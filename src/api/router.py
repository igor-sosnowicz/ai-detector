"""Module with a main router."""

from collections.abc import AsyncGenerator
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Request
from fastapi.routing import APIRouter
from loguru import logger

from src.api.data_models import (
    EvaluationMethod,
    HealthcheckResponse,
    LLMDetectionRequest,
    LLMDetectionResponse,
)
from src.api.rate_limiter import RateLimiter
from src.api.utils import (
    get_ip_address_or_raise,
    get_length_adjusted_confidence,
    map_score_to_confidence,
)
from src.detection.compression_based import CompressionBasedDetector
from src.detection.heuristics import Heuristics
from src.detection.llm_based import calculate_frequency_of_rule_of_three
from src.detection.spelling import SpellingDetector
from src.ml.stylometric_classifier import StylometricClassifier

router = APIRouter()
rate_limiter = RateLimiter()

english_heuristics = Heuristics(language="english")
perplexity_detector = CompressionBasedDetector()
stylometric_classifier = StylometricClassifier()
english_spelling = SpellingDetector()


@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:  # noqa: ARG001, The signature of the function cannot be changed but `app` is not needed.
    """Perform startup and shutdown chores associated with the API."""
    await stylometric_classifier.prepare_model()

    yield


@router.get("/health")
async def check_health(fastapi_request: Request) -> HealthcheckResponse:
    """
    Check if the API function well.

    ## Returns

    **HealthcheckResponse**: Report on health status of the API and its subsystems.
    """
    ip_address = get_ip_address_or_raise(fastapi_request)
    rate_limiter(identifier=ip_address)

    return HealthcheckResponse(
        is_healthy=True,
    )


@router.post("/is_ai")
async def is_llm_generated(
    request: LLMDetectionRequest, fastapi_request: Request
) -> LLMDetectionResponse:
    """
    Find out whether a text was written by an LLM or a human being.

    ## Parameters

    **request** (LLMDetectionRequest): Request for examining origin of a document:
        either LLM_based or human-based. Contains the following fields:

    - **text** (str): Text to be evaluate. The longer text,
        the more reliable result.
    - **method** (EvaluationMethod): Type of the evaluation method.
        Evaluation methods vary by their accuracy and inference speed.
        More accurate methods are usually slower.
        Defaults to "most_accurate".

    ## Returns

    **LLMDetectionResponse**: Results of the document origin evaluation.
        It contains the following fields:

    - **is_llm_generated** (bool): Whether a text was generated by an LLM
        or not.
    - **confidence** (float): The confidence that it was LLM generated.
        The higher confidence, the more certain the system is that
        the text was LLM-generated. The value is always in the range [0, 1].
    """
    ip_address = get_ip_address_or_raise(fastapi_request)
    rate_limiter(identifier=ip_address)

    score = 0.0
    threshold = 0.0
    remark = None

    match request.method:
        case EvaluationMethod.FASTEST:
            score = english_heuristics.detect(request.text, "english")
            threshold = english_heuristics.get_threshold()

        case EvaluationMethod.FAST:
            score = english_spelling.detect(request.text, "english")
            threshold = english_spelling.get_threshold()

        case EvaluationMethod.BALANCED:
            score = perplexity_detector.detect(request.text, "english")
            threshold = perplexity_detector.get_threshold()

        case EvaluationMethod.MORE_ACCURATE:
            score = await calculate_frequency_of_rule_of_three(request.text)
            threshold = 0.65

        case EvaluationMethod.MOST_ACCURATE:
            score = stylometric_classifier.detect(request.text, "english")
            threshold = stylometric_classifier.get_threshold()

        case _:
            raise HTTPException(status_code=500, detail="A method is invalid.")

    confidence, remark = get_length_adjusted_confidence(
        confidence=score, text=request.text
    )
    confidence = map_score_to_confidence(confidence)

    logger.info(f"Score: {score}\n\nText: {request.text}")

    return LLMDetectionResponse(
        is_llm_generated=score >= threshold,
        confidence=confidence,
        remarks=remark if isinstance(remark, str) else "No remarks.",
    )
