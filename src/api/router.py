"""Module with a main router."""

from collections.abc import AsyncGenerator
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Request
from fastapi.routing import APIRouter
from loguru import logger

from src.api.data_models import (
    EvaluationMethod,
    HealthcheckResponse,
    LLMDetectionRequest,
    LLMDetectionResponse,
)
from src.api.rate_limiter import RateLimiter
from src.api.utils import (
    get_ip_address_or_raise,
    get_length_adjusted_confidence,
    map_score_to_confidence,
)
from src.detection.compression_based import CompressionBasedDetector
from src.detection.heuristics import Heuristics
from src.detection.llm_based import calculate_frequency_of_rule_of_three
from src.ml.stylometric_classifier import StylometricClassifier

router = APIRouter()
rate_limiter = RateLimiter()

english_heuristics = Heuristics(language="english")
perplexity_detector = CompressionBasedDetector()
# language_detector =
stylometric_classifier = StylometricClassifier()


@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:  # noqa: ARG001, The signature of the function cannot be changed but `app` is not needed.
    """Perform startup and shutdown chores associated with the API."""
    await stylometric_classifier.prepare_model()
    logger.debug(
        "Test accuracy of stylometric classifier: "
        f"{await stylometric_classifier.test()}"
    )
    yield


@router.get("/health")
async def check_health() -> HealthcheckResponse:
    """
    Check if the API function well.

    ## Returns

    **HealthcheckResponse**: Report on health status of the API and its subsystems.
    """
    return HealthcheckResponse(
        is_healthy=True,
    )


@router.post("/is_ai")
async def is_llm_generated(
    request: LLMDetectionRequest, fastapi_request: Request
) -> LLMDetectionResponse:
    """
    Find out whether a text was written by an LLM or a human being.

    ## Parameters
        **request** (LLMDetectionRequest): Request for examining origin of a document:
            either LLM_based or human-based. Contains the following fields:
                - **text** (str): Text to be evaluate. The longer text,
                    the more reliable result.
                - **method** (EvaluationMethod): Type of the evaluation method.
                    Evaluation methods vary by their accuracy and inference speed.
                    More accurate methods are usually slower.
                    Defaults to "most_accurate".

    Raises:
        **HTTPException**: Raised when an unsupported value of the **method** was
            provided.

    Returns:
        **LLMDetectionResponse**: Results of the document origin evaluation.
            It contains the following fields:
            - **is_llm_generated** (bool): Whether a text was generated by an LLM
                or not.
            - **confidence** (float): The confidence that it was LLM generated.
                The higher confidence, the more certain the system is that
                the text was LLM-generated. The value is always in the range [0, 1].
    """
    ip_address = get_ip_address_or_raise(fastapi_request)
    rate_limiter(identifier=ip_address)

    score = 0.0
    threshold = 0.0
    remark = None

    match request.method:
        case EvaluationMethod.FASTEST:
            score = english_heuristics.detect(request.text)
            threshold = 0.6

        case EvaluationMethod.FAST:
            raise HTTPException(
                status_code=500, detail="A method is not supported yet."
            )

        case EvaluationMethod.BALANCED:
            score = perplexity_detector.detect(request.text, "english")
            threshold = 1.0

        case EvaluationMethod.MORE_ACCURATE:
            score = await calculate_frequency_of_rule_of_three(request.text)
            threshold = 0.65

        case EvaluationMethod.MOST_ACCURATE:
            score = stylometric_classifier(request.text)
            threshold = 0.6

        case _:
            raise HTTPException(status_code=500, detail="A method is invalid.")

    confidence, remark = get_length_adjusted_confidence(
        confidence=score, text=request.text
    )
    confidence = map_score_to_confidence(confidence)

    return LLMDetectionResponse(
        is_llm_generated=score >= threshold,
        confidence=confidence,
        remarks=remark if isinstance(remark, str) else "No remarks.",
    )
